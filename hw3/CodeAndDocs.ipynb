{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Big Data Analytics - Assignment 3  \n",
    "# Programming with GPUs using CUDA and pyCUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up  and verifying the environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I followed the instructions on [this link](http://tleyden.github.io/blog/2014/10/25/cuda-6-dot-5-on-aws-gpu-instance-running-ubuntu-14-dot-04/) to first get an Amazon Machine Image with CUDA installed on it.  \n",
    "- On searching for an AMI with the ID `ami-2cbf3e44`, 2 images show up, one which just has CUDA installed on it and the other with all the Python dependencies required to install pyCUDA bundled in as well. I chose the latter one with all the Python dependencies installed.\n",
    "- Once the machine was set up, we have to install pyCUDA. To install pyCUDA, I followed the instructions provided on the [pyCUDA Ubuntu Installation Page](https://wiki.tiker.net/PyCuda/Installation/Linux/Ubuntu) to get pyCUDA set up and ready to rumble.  \n",
    "\n",
    "To install pyCUDA correctly, we run the following commands:  \n",
    "- First download the newest version of pyCUDA and unpack it\n",
    "```shell\n",
    "$ wget https://pypi.python.org/packages/source/p/pycuda/pycuda-2016.1.tar.gz\n",
    "$ tar zxvf pycuda-2016.1.tar.gz\n",
    "```\n",
    "- Then, go into the unpacked directory and set up the configuration file\n",
    "```shell\n",
    "$ cd pycuda-2016.1\n",
    "```\n",
    "    \n",
    "    - Since we are using a 64 bit machine with Python 2.7, we have to run the following command to set the configuration correctly\n",
    "    ```shell\n",
    "    $ ./configure.py --cuda-root=/usr/local/cuda --cudadrv-lib-dir=/usr/lib/x86_64-linux-gnu --boost-inc-dir=/usr/include --boost-lib-dir=/usr/lib --boost-python-libname=boost_python --boost-thread-libname=boost_thread --no-use-shipped-boost\n",
    "    ```\n",
    "    \n",
    "- Then simply make and install the package\n",
    "```shell\n",
    "$ make -j 4\n",
    "$ sudo python setup.py install\n",
    "$ sudo pip install .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the installation, we run the example of pyCUDA on the landing page of the documentation (`pycuda_docs_hello.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pycuda.autoinit\n",
    "import pycuda.driver as drv\n",
    "import numpy\n",
    "\n",
    "from pycuda.compiler import SourceModule\n",
    "mod = SourceModule(\"\"\"\n",
    "__global__ void multiply_them(float *dest, float *a, float *b)\n",
    "{\n",
    "  const int i = threadIdx.x;\n",
    "  dest[i] = a[i] * b[i];\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "multiply_them = mod.get_function(\"multiply_them\")\n",
    "\n",
    "a = numpy.random.randn(400).astype(numpy.float32)\n",
    "b = numpy.random.randn(400).astype(numpy.float32)\n",
    "\n",
    "dest = numpy.zeros_like(a)\n",
    "multiply_them(\n",
    "        drv.Out(dest), drv.In(a), drv.In(b),\n",
    "        block=(400,1,1), grid=(1,1))\n",
    "print dest - a*b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code simply returns a 2D array of all 0s, as it computes the dot product of the 2 matrices `a` and `b` using the GPU and simply as a `numpy` function, and prints the difference between the 2.  \n",
    "Seeing all 0s makes it easy to verify that both computations gave the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing and Running a more complicated example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is adapted from the [Tiled Matrix Multiplication](https://wiki.tiker.net/PyCuda/Examples/MatrixmulTiled) code on the pyCUDA wiki.  \n",
    "The code is documented with comments wherever necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "\"\"\" \n",
    "This code is adapted from the example found at : https://wiki.tiker.net/PyCuda/Examples/MatrixmulTiled\n",
    "\n",
    "The program takes 2 square matrices as numpy arrays, \n",
    "and multiplies them together using multiple blocks and shared memory. \n",
    "Each thread block is assigned a \"tile\" of the resulting matrix and is responsible\n",
    "for generating the elements in that tile.  Each thread in a block computes one element \n",
    "of the tile.\n",
    "\n",
    "We also compute the difference in the result when the calculations are done on\n",
    "the CPU vs the GPU using the gpuarray module of pycuda\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "from pycuda import driver, compiler, gpuarray, tools\n",
    "\n",
    "# -- initialize the device\n",
    "import pycuda.autoinit\n",
    "\n",
    "# the C code that will run as a multithreaded process on the GPU\n",
    "kernel_code_template = \"\"\"\n",
    "__global__ void MatrixMulKernel(float *A, float *B, float *C)\n",
    "{\n",
    "\n",
    "  const uint wA = %(MATRIX_SIZE)s;\n",
    "  const uint wB = %(MATRIX_SIZE)s;  \n",
    "  \n",
    "  // Block index\n",
    "  const uint bx = blockIdx.x;\n",
    "  const uint by = blockIdx.y;\n",
    "\n",
    "  // Thread index\n",
    "  const uint tx = threadIdx.x;\n",
    "  const uint ty = threadIdx.y;\n",
    "\n",
    "  // Index of the first sub-matrix of A processed by the block\n",
    "  const uint aBegin = wA * %(BLOCK_SIZE)s * by;\n",
    "  // Index of the last sub-matrix of A processed by the block\n",
    "  const uint aEnd = aBegin + wA - 1;\n",
    "  // Step size used to iterate through the sub-matrices of A\n",
    "  const uint aStep = %(BLOCK_SIZE)s;\n",
    "\n",
    "  // Index of the first sub-matrix of B processed by the block\n",
    "  const uint bBegin = %(BLOCK_SIZE)s * bx;\n",
    "  // Step size used to iterate through the sub-matrices of B\n",
    "  const uint bStep = %(BLOCK_SIZE)s * wB;\n",
    "\n",
    "  // The element of the block sub-matrix that is computed\n",
    "  // by the thread\n",
    "  float Csub = 0;\n",
    "  // Loop over all the sub-matrices of A and B required to\n",
    "  // compute the block sub-matrix\n",
    "  for (int a = aBegin, b = bBegin;\n",
    "       a <= aEnd;\n",
    "       a += aStep, b += bStep) \n",
    "    {\n",
    "      // Shared memory for the sub-matrix of A\n",
    "      __shared__ float As[%(BLOCK_SIZE)s][%(BLOCK_SIZE)s];\n",
    "      // Shared memory for the sub-matrix of B\n",
    "      __shared__ float Bs[%(BLOCK_SIZE)s][%(BLOCK_SIZE)s];\n",
    "\n",
    "      // Load the matrices from global memory to shared memory\n",
    "      // each thread loads one element of each matrix\n",
    "      As[ty][tx] = A[a + wA * ty + tx];\n",
    "      Bs[ty][tx] = B[b + wB * ty + tx];\n",
    "      // Synchronize to make sure the matrices are loaded\n",
    "      __syncthreads();\n",
    "\n",
    "      // Multiply the two matrices together;\n",
    "      // each thread computes one element\n",
    "      // of the block sub-matrix\n",
    "      for (int k = 0; k < %(BLOCK_SIZE)s; ++k)\n",
    "        Csub += As[ty][k] * Bs[k][tx];\n",
    "\n",
    "      // Synchronize to make sure that the preceding\n",
    "      // computation is done before loading two new\n",
    "      // sub-matrices of A and B in the next iteration\n",
    "      __syncthreads();\n",
    "    }\n",
    "\n",
    "  // Write the block sub-matrix to global memory;\n",
    "  // each thread writes one element\n",
    "  const uint c = wB * %(BLOCK_SIZE)s * by + %(BLOCK_SIZE)s * bx;\n",
    "  C[c + wB * ty + tx] = Csub;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# define the (square) matrix size\n",
    "# Since we want to work with matrices of size 50 MB or so, we need to do some calculations\n",
    "# to determine what the size of the input sqaure matrices will be\n",
    "'''\n",
    "1 float = 8 bits\n",
    "50 mb = 50 * 1024 * 1024\n",
    "Total number of floats required in matrices = 50 * 1024 * 1024 / 8\n",
    "\n",
    "Using this, the size of the matrix = sqrt(50 * 1024 * 1024 / 8) = 2560\n",
    "Hence, to manipulate matrices of size 50 mb each, \n",
    "we need the total number of elements in the matrix to be 2560 x 2560\n",
    "therefore, set the size of the matrix as 2560\n",
    "'''\n",
    "MATRIX_SIZE = 2560 \n",
    "\n",
    "# define size of blocks and tiles sub-matrix \n",
    "# (we assume that the block size is same as tile size)\n",
    "# pycuda will assign a particular number of threads to each block\n",
    "# checking the output of dump_properties.py file, we see that we can have at most\n",
    "# 1024 threads per block\n",
    "# as long as the total number of elements in a tile are less than 1024, pycuda can execute this code without any problems \n",
    "TILE_SIZE = 32 \n",
    "BLOCK_SIZE = TILE_SIZE\n",
    "\n",
    "# create two random square matrices, each of size 50 mb\n",
    "a_cpu = np.random.randn(MATRIX_SIZE, MATRIX_SIZE).astype(np.float32)\n",
    "b_cpu = np.random.randn(MATRIX_SIZE, MATRIX_SIZE).astype(np.float32)\n",
    "\n",
    "# compute reference on the CPU to verify GPU computation\n",
    "c_cpu = np.dot(a_cpu, b_cpu)\n",
    "\n",
    "# transfer host (CPU) memory to device (GPU) memory \n",
    "a_gpu = gpuarray.to_gpu(a_cpu) \n",
    "b_gpu = gpuarray.to_gpu(b_cpu)\n",
    "\n",
    "# create empty gpu array for the result (C = A * B)\n",
    "c_gpu = gpuarray.empty((MATRIX_SIZE, MATRIX_SIZE), np.float32)\n",
    "\n",
    "# get the kernel code from the template \n",
    "# by specifying the constants MATRIX_SIZE and BLOCK_SIZE\n",
    "kernel_code = kernel_code_template % { \n",
    "    'MATRIX_SIZE': MATRIX_SIZE,\n",
    "    'BLOCK_SIZE': BLOCK_SIZE,\n",
    "    }\n",
    "\n",
    "# compile the kernel code\n",
    "mod = compiler.SourceModule(kernel_code)\n",
    "\n",
    "# get the kernel function from the compiled module\n",
    "matrixmul = mod.get_function(\"MatrixMulKernel\")\n",
    "\n",
    "# call the kernel on the Graphics card\n",
    "matrixmul(\n",
    "    # inputs\n",
    "    a_gpu, b_gpu, \n",
    "    # output\n",
    "    c_gpu, \n",
    "    # grid of multiple blocks\n",
    "    grid = (MATRIX_SIZE // TILE_SIZE, MATRIX_SIZE // TILE_SIZE),\n",
    "    # block of multiple threads\n",
    "    block = (TILE_SIZE, TILE_SIZE, 1), \n",
    "    )\n",
    "\n",
    "# print the results\n",
    "print \"-\" * 80\n",
    "print \"Matrix A (GPU):\"\n",
    "print a_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Matrix B (GPU):\"\n",
    "print b_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"Matrix C (GPU):\"\n",
    "print c_gpu.get()\n",
    "\n",
    "print \"-\" * 80\n",
    "print \"CPU-GPU difference:\"\n",
    "print c_cpu - c_gpu.get()\n",
    "print \"L2 norm:\", la.norm(c_cpu - c_gpu.get())\n",
    "np.allclose(c_cpu, c_gpu.get())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion : \n",
    "There are some important things to note here (also mentioned in code comments):\n",
    "- Since we want to deal with data that is at least 50 MB in size, we first need to calculate how many elements we will need in each matrix to make it require 50 MB of memory. To find the exact number, we need to do some calculations:\n",
    "\n",
    "```\n",
    "1 float = 8 bits\n",
    "50 mb = 50 * 1024 * 1024\n",
    "Total number of floats required in matrices = 50 * 1024 * 1024 / 8\n",
    "\n",
    "Using this, the size of the matrix = sqrt(50 * 1024 * 1024 / 8) = 2560\n",
    "Hence, to manipulate matrices of size 50 mb each, \n",
    "we need the total number of elements in the matrix to be 2560 x 2560\n",
    "which means that we have to set the MATRIX_SIZE parameter as 2560\n",
    "```\n",
    "\n",
    "- The tile size cannot be arbitrarily large.  \n",
    "    Each thread block is assigned a \"tile\" of the resulting matrix and is responsible\n",
    "    for generating the elements in that tile.  Each thread in a block computes one element \n",
    "    of the tile. The question then is how to choose a good tile size.\n",
    "    \n",
    "In the pyCUDA examples directory in the unpacked `pycuda-2016.1` folder, there is a file called `dump_properties.py`, which simply prints out all of the default and user defined properties of pyCUDA and the underlying CUDA installation.\n",
    "In this dump, we see that the `MAX_THREAD_BLOCK_SIZE` is set to 1024. i.e., there can be at most 1024 separate threads for each block.  \n",
    "Each thread block deals with 1 tile, and each thread in the block is responsible for computing the value of one element in the tile.  \n",
    "The `TILE_SIZE` parameter in the code is responsible for making a thread block of size `TILE_SIZE x TILE_SIZE`.  \n",
    "Therefore, it can be set to a maximum of `sqrt(1024) = 32`  \n",
    "Since we are dealing with very very large matrices, it makes sense to use mulithreading in total force and set the tile size to 32.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On running the above program, we get the following output in the shell, which prints the 2 matrices `a` and `b` and the result of the computation `a * b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\r\n",
      "Matrix A (GPU):\r\n",
      "[[ 0.01775491  0.70539278 -0.87263292 ...,  0.2844947  -0.21808594\r\n",
      "   1.21563804]\r\n",
      " [-0.6686821   0.41078544  1.21795642 ...,  0.1043731   0.07380684\r\n",
      "  -0.73897499]\r\n",
      " [-0.94774461  2.66338634 -0.33823362 ..., -0.49412024  1.73328018\r\n",
      "  -1.50323164]\r\n",
      " ..., \r\n",
      " [-1.79621673  1.41672575  1.84087658 ...,  1.06714141  0.70821875\r\n",
      "   2.42188454]\r\n",
      " [-0.21768838  0.40100154 -1.41286051 ...,  0.85705167 -0.44164455\r\n",
      "   0.40237099]\r\n",
      " [-0.99681801 -0.03101943 -0.63504004 ..., -0.44020164 -0.47245443\r\n",
      "  -0.05559199]]\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Matrix B (GPU):\r\n",
      "[[ 0.87681538 -1.21908629  1.6373837  ..., -0.11167652  0.09793786\r\n",
      "  -0.60445577]\r\n",
      " [ 2.1978476   0.8985244   0.28708866 ...,  0.2088144  -0.16963698\r\n",
      "  -0.74097604]\r\n",
      " [-0.61910552  0.11249758  2.48630381 ...,  0.48025823  0.1537658\r\n",
      "  -0.41130912]\r\n",
      " ..., \r\n",
      " [ 0.36054409  1.8271873   1.74100745 ..., -0.94321477  1.77258706\r\n",
      "  -0.17511301]\r\n",
      " [-2.22853136 -0.22352615 -1.77978384 ...,  1.74593329 -1.0405736\r\n",
      "   1.46153355]\r\n",
      " [-0.26703045 -1.12884033 -0.37428653 ..., -1.51433372  1.71066332\r\n",
      "  -1.65709507]]\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Matrix C (GPU):\r\n",
      "[[  52.46976089   27.45445442    5.22401714 ...,  -24.92034912\r\n",
      "    57.36989212   26.61909676]\r\n",
      " [ -14.30520344  -20.782341     59.41819    ...,   -4.95890522\r\n",
      "    50.66709137  -91.21901703]\r\n",
      " [ -65.88220215  -26.92414284   22.4103241  ...,   84.48514557\r\n",
      "   -19.01320648   -6.38791609]\r\n",
      " ..., \r\n",
      " [ -96.5242691   -30.58050728  -60.24150467 ...,   37.91069794\r\n",
      "    15.02737427  -28.25194931]\r\n",
      " [ -31.9144249   -16.0401535     3.33400154 ...,  -30.85684204   17.3628273\r\n",
      "   103.04785156]\r\n",
      " [  49.60402679   33.84770203  -30.31701088 ...,   85.62138367\r\n",
      "   -13.76645088   57.07614136]]\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "CPU-GPU difference:\r\n",
      "[[ -3.81469727e-05  -2.86102295e-05  -2.43186951e-05 ...,   2.09808350e-05\r\n",
      "   -7.62939453e-06   4.00543213e-05]\r\n",
      " [  8.58306885e-06   6.86645508e-05   3.81469727e-05 ...,   2.28881836e-05\r\n",
      "    3.43322754e-05   1.14440918e-04]\r\n",
      " [  4.57763672e-05  -3.81469727e-05  -9.53674316e-06 ...,  -4.57763672e-05\r\n",
      "    3.43322754e-05  -9.53674316e-07]\r\n",
      " ..., \r\n",
      " [  5.34057617e-05   1.71661377e-05  -3.81469727e-06 ...,   3.81469727e-06\r\n",
      "    4.76837158e-06   1.14440918e-05]\r\n",
      " [ -3.05175781e-05   5.72204590e-06  -3.31401825e-05 ...,   0.00000000e+00\r\n",
      "   -1.90734863e-06   4.57763672e-05]\r\n",
      " [  1.52587891e-05  -4.19616699e-05   7.62939453e-06 ...,   7.62939453e-06\r\n",
      "   -3.81469727e-06  -7.62939453e-06]]\r\n",
      "L2 norm: 0.11035\r\n"
     ]
    }
   ],
   "source": [
    "cat output.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can see that the CPU - GPU difference is fairly low for the individual terms (order of 10e-5 or 10e-6).\n",
    "MSE of CPU-GPU difference  = L2 Norm = 0.11035  \n",
    "This is a very small value given that we are taking the L2 norm over 2560 * 2560 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
